{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuQq5I5iISqWnJAym2qPg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedAhmedGalal/Spark-Apache-favorita-grocery-sal-es-forecasting-/blob/main/Feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kaggle library installation**\n",
        "To be able to import required dataset through api credentials"
      ],
      "metadata": {
        "id": "xmSYW3MRIYdP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leD-n3-IQwIs",
        "outputId": "eaa81de7-b939-4687-bcda-c6524f05b739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **API credentials uploading**\n",
        "Required API file was downloadedd from my kaggle account, just to be able to use kaggle datasets"
      ],
      "metadata": {
        "id": "NfSNsc6NImGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "kiTThQ4QR2LD",
        "outputId": "b9d2305d-166a-4e61-df91-15ef3a9e1622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-830cf188-b623-4a4f-82bd-e0446ecd0d15\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-830cf188-b623-4a4f-82bd-e0446ecd0d15\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mawkley\",\"key\":\"63bc38bb5e5b333a6e7dbd814357d370\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting up Kaggle API access in Colab**\n",
        "First, !mkdir -p ~/.kaggle creates a hidden .kaggle folder where the API key must be stored. Then, !mv kaggle.json ~/.kaggle/ moves  downloaded Kaggle credentials (kaggle.json) into that folder. Finally, !chmod 600 ~/.kaggle/kaggle.json secures the file by allowing only the owner to read and write it, preventing others from accessing your private key. Together, they ensure the Kaggle CLI can authenticate safely."
      ],
      "metadata": {
        "id": "lXPyfFi8I3oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "d2WvMMG9SEKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download required avorita-grocery-sales-forecasting dataset from kaggle**"
      ],
      "metadata": {
        "id": "dPrMXxFvJUo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c favorita-grocery-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AnMeg5bSNHd",
        "outputId": "bc4b8153-3c6d-4634-c939-680d2bdf7685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading favorita-grocery-sales-forecasting.zip to /content\n",
            " 97% 446M/458M [00:00<00:00, 386MB/s]\n",
            "100% 458M/458M [00:01<00:00, 477MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset file unzipping**"
      ],
      "metadata": {
        "id": "lXbvrE0pJbql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/favorita-grocery-sales-forecasting.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Rz8sxDSpEc",
        "outputId": "2a82df3f-c1f3-4bc2-f3d8-a0c3277d42d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/favorita-grocery-sales-forecasting.zip\n",
            "  inflating: holidays_events.csv.7z  \n",
            "  inflating: items.csv.7z            \n",
            "  inflating: oil.csv.7z              \n",
            "  inflating: sample_submission.csv.7z  \n",
            "  inflating: stores.csv.7z           \n",
            "  inflating: test.csv.7z             \n",
            "  inflating: train.csv.7z            \n",
            "  inflating: transactions.csv.7z     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unzipping required CSV files Train,stores and items**"
      ],
      "metadata": {
        "id": "rV5jPiv9JmGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install p7zip-full\n",
        "!7z x /content/train.csv.7z\n",
        "!7z x /content/stores.csv.7z\n",
        "!7z x /content/items.csv.7z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyEiTIQtVYji",
        "outputId": "339895a1-fdf5-42f6-fb7e-1bda9891b6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 474092593 bytes (453 MiB)\n",
            "\n",
            "Extracting archive: /content/train.csv.7z\n",
            "--\n",
            "Path = /content/train.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 474092593\n",
            "Headers Size = 122\n",
            "Method = LZMA2:24\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100% 1\b\b\b\b\b\b      \b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       4997452288\n",
            "Compressed: 474092593\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 648 bytes (1 KiB)\n",
            "\n",
            "Extracting archive: /content/stores.csv.7z\n",
            "--\n",
            "Path = /content/stores.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 648\n",
            "Headers Size = 130\n",
            "Method = LZMA2:12\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       1387\n",
            "Compressed: 648\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 14315 bytes (14 KiB)\n",
            "\n",
            "Extracting archive: /content/items.csv.7z\n",
            "--\n",
            "Path = /content/items.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 14315\n",
            "Headers Size = 122\n",
            "Method = LZMA2:17\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       101841\n",
            "Compressed: 14315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pyspark installation**"
      ],
      "metadata": {
        "id": "3A66hUQLJh44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXAg2ttNUVv-",
        "outputId": "a8c7924b-9a3e-4fe2-aad6-376ec38a32da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading CSV files within dataset using Pyspark**\n",
        "Spark session start,then reading dataset files"
      ],
      "metadata": {
        "id": "agoaFZzFJwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"FavoritaForecast\").getOrCreate()\n",
        "train = spark.read.csv(\"/content/train.csv\", header=True, inferSchema=True)\n",
        "items = spark.read.csv(\"/content/items.csv\", header=True, inferSchema=True)\n",
        "stores = spark.read.csv(\"/content/stores.csv\", header=True, inferSchema=True)\n",
        "train.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfxRSWoZV6Fb",
        "outputId": "f18a03eb-2fb8-447d-9ee6-cb6af743387c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+--------+----------+-----------+\n",
            "| id|      date|store_nbr|item_nbr|unit_sales|onpromotion|\n",
            "+---+----------+---------+--------+----------+-----------+\n",
            "|  0|2013-01-01|       25|  103665|       7.0|       NULL|\n",
            "|  1|2013-01-01|       25|  105574|       1.0|       NULL|\n",
            "|  2|2013-01-01|       25|  105575|       2.0|       NULL|\n",
            "|  3|2013-01-01|       25|  108079|       1.0|       NULL|\n",
            "|  4|2013-01-01|       25|  108701|       1.0|       NULL|\n",
            "+---+----------+---------+--------+----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data types consistency guaranteeing **\n",
        "  According to required within test,date column in the train DataFrame is converted into a proper DateType, casts unit_sales to FloatType for numerical operations, and changes onpromotion into a Boolean flag. Both store_nbr and item_nbr are cast to IntegerType in all relevant DataFrames (train, items, and stores) to guarantee that joins and aggregations work correctly without type mismatches. This preprocessing step is essential for clean, reliable downstream analysis and modeling."
      ],
      "metadata": {
        "id": "qzwZ_X7pKEkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType, FloatType, IntegerType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "train = (\n",
        "    train\n",
        "    .withColumn(\"date\", col(\"date\").cast(DateType()))\n",
        "    .withColumn(\"unit_sales\", col(\"unit_sales\").cast(FloatType()))\n",
        "    .withColumn(\"onpromotion\", col(\"onpromotion\").cast(\"boolean\"))\n",
        "    .withColumn(\"store_nbr\", col(\"store_nbr\").cast(IntegerType()))\n",
        "    .withColumn(\"item_nbr\", col(\"item_nbr\").cast(IntegerType()))\n",
        ")\n",
        "\n",
        "items = (\n",
        "    items\n",
        "    .withColumn(\"item_nbr\", col(\"item_nbr\").cast(IntegerType()))\n",
        ")\n",
        "\n",
        "stores = (\n",
        "    stores\n",
        "    .withColumn(\"store_nbr\", col(\"store_nbr\").cast(IntegerType()))\n",
        ")\n"
      ],
      "metadata": {
        "id": "2Ii5mKzRZBMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset files (tables) joining**\n",
        "Train is to be joined with items according to item_nbr feature (column), then joining created dataframe with stores table according to store_nbr feature (column). After joining, the head of created dataframe is checked to comply requiredd joining properly."
      ],
      "metadata": {
        "id": "EbysHmszKdaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train.join(items, on=\"item_nbr\", how=\"left\")\n",
        "\n",
        "# df + stores on store_nbr\n",
        "df = df.join(stores, on=\"store_nbr\", how=\"left\")\n",
        "\n",
        "# --- Step 6: Quick check ---\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccjUJ2_SZCqe",
        "outputId": "9f91c891-c384-46ff-eaf2-a9d5caaafee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- store_nbr: integer (nullable = true)\n",
            " |-- item_nbr: integer (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- unit_sales: float (nullable = true)\n",
            " |-- onpromotion: boolean (nullable = true)\n",
            " |-- family: string (nullable = true)\n",
            " |-- class: integer (nullable = true)\n",
            " |-- perishable: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- cluster: integer (nullable = true)\n",
            "\n",
            "+---------+--------+---+----------+----------+-----------+------------+-----+----------+-------+-----------+----+-------+\n",
            "|store_nbr|item_nbr| id|      date|unit_sales|onpromotion|      family|class|perishable|   city|      state|type|cluster|\n",
            "+---------+--------+---+----------+----------+-----------+------------+-----+----------+-------+-----------+----+-------+\n",
            "|       25|  103665|  0|2013-01-01|       7.0|       NULL|BREAD/BAKERY| 2712|         1|Salinas|Santa Elena|   D|      1|\n",
            "|       25|  105574|  1|2013-01-01|       1.0|       NULL|   GROCERY I| 1045|         0|Salinas|Santa Elena|   D|      1|\n",
            "|       25|  105575|  2|2013-01-01|       2.0|       NULL|   GROCERY I| 1045|         0|Salinas|Santa Elena|   D|      1|\n",
            "|       25|  108079|  3|2013-01-01|       1.0|       NULL|   GROCERY I| 1030|         0|Salinas|Santa Elena|   D|      1|\n",
            "|       25|  108701|  4|2013-01-01|       1.0|       NULL|        DELI| 2644|         1|Salinas|Santa Elena|   D|      1|\n",
            "+---------+--------+---+----------+----------+-----------+------------+-----+----------+-------+-----------+----+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import (\n",
        "    col, lag, avg, stddev, dayofweek, when\n",
        ")\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Define the base window (partition by store/item, order by date)\n",
        "w = Window.partitionBy(\"store_nbr\", \"item_nbr\").orderBy(\"date\")\n",
        "\n",
        "# --- Step 1: Lag features (safe: use past rows only) ---\n",
        "df = df.withColumn(\"lag_1\", F.lag(\"unit_sales\", 1).over(w))\n",
        "df = df.withColumn(\"lag_7\", F.lag(\"unit_sales\", 7).over(w))\n",
        "\n",
        "# --- Step 2: Rolling features (calendar-based, exclude current day) ---\n",
        "# Rolling mean over previous 7 rows (days), excluding current row\n",
        "df = df.withColumn(\n",
        "    \"rolling_mean_7\",\n",
        "    F.avg(\"unit_sales\").over(w.rowsBetween(-7, -1))\n",
        ")\n",
        "\n",
        "# Rolling std over previous 14 rows (days), excluding current row\n",
        "df = df.withColumn(\n",
        "    \"rolling_std_14\",\n",
        "    F.stddev(\"unit_sales\").over(w.rowsBetween(-14, -1))\n",
        ")\n",
        "# --- Step 3: Calendar-based features ---\n",
        "df = (\n",
        "    df.withColumn(\"day_of_week\", F.dayofweek(\"date\") - 2)  # Spark: Sunday=1, so shift → Monday=0\n",
        "      .withColumn(\"day_of_week\", F.when(F.col(\"day_of_week\") < 0, 6).otherwise(F.col(\"day_of_week\")))\n",
        "      .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([5, 6]), 1).otherwise(0))\n",
        ")\n",
        "\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raDiXmY9Zc7-",
        "outputId": "e66602c9-7f56-4737-cf92-2f3c683177e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+--------+----------+----------+-----------+---------+-----+----------+-----+---------+----+-------+-----+-----+------------------+------------------+-----------+----------+\n",
            "|store_nbr|item_nbr|      id|      date|unit_sales|onpromotion|   family|class|perishable| city|    state|type|cluster|lag_1|lag_7|    rolling_mean_7|    rolling_std_14|day_of_week|is_weekend|\n",
            "+---------+--------+--------+----------+----------+-----------+---------+-----+----------+-----+---------+----+-------+-----+-----+------------------+------------------+-----------+----------+\n",
            "|        1|  108634|11974039|2013-10-03|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13| NULL| NULL|              NULL|              NULL|          3|         0|\n",
            "|        1|  108634|12019808|2013-10-04|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  1.0| NULL|               1.0|              NULL|          4|         0|\n",
            "|        1|  108634|12066191|2013-10-05|       2.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  1.0| NULL|               1.0|               0.0|          5|         1|\n",
            "|        1|  108634|12115669|2013-10-06|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  2.0| NULL|1.3333333333333333|0.5773502691896258|          6|         1|\n",
            "|        1|  108634|12163834|2013-10-07|       4.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  1.0| NULL|              1.25|               0.5|          0|         0|\n",
            "|        1|  108634|12209714|2013-10-08|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  4.0| NULL|               1.8|1.3038404810405297|          1|         0|\n",
            "|        1|  108634|12343879|2013-10-11|       2.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  1.0| NULL|1.6666666666666667|1.2110601416389968|          4|         0|\n",
            "|        1|  108634|12390677|2013-10-12|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  2.0|  1.0|1.7142857142857142|1.1126972805283737|          5|         1|\n",
            "|        1|  108634|12485823|2013-10-14|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  1.0|  1.0|1.7142857142857142|1.0606601717798212|          0|         0|\n",
            "|        1|  108634|12808804|2013-10-21|       1.0|       NULL|GROCERY I| 1075|         0|Quito|Pichincha|   D|     13|  1.0|  2.0|1.7142857142857142|1.0137937550497031|          0|         0|\n",
            "+---------+--------+--------+----------+----------+-----------+---------+-----+----------+-----+---------+----+-------+-----+-----+------------------+------------------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=[\"lag_1\", \"lag_7\", \"rolling_mean_7\", \"rolling_std_14\"])"
      ],
      "metadata": {
        "id": "p0WFoIHccGjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking that day names and numbers within week are mapped correctly,but scanning this large dataset from spark takes a lot of time,it is better to take a sample from data to check on\n",
        "It appeared that numbering is ok, and is_weekend is ok, as i have manually checked that sample day_of_week are consistent with date, and that days that are labeled is_weekend are actyally saturdays and sundays\n",
        "\n"
      ],
      "metadata": {
        "id": "HqB_aIh2cnNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Leakage Defense\n",
        "\n",
        "**What is data leakage in time-series features?**  \n",
        "Data leakage occurs when information from the future which should be forecasted first \"leaks\" into the model during training. That is to say, forecasted data happening in future should be predicted not trained on, however, the model design had a leakage that caused training on predicted data.\n",
        "For time-series problems, this leakage  occurs if lag or rolling features according to their design and definition use values from the same day or future dates.  \n",
        "This misleads model expected performance, as it will perform very good while training but fails after production when using new test or validation datan.\n",
        "# **Design-Time Leakage Prevention Guarantees**\n",
        "**How we prevented leakage in this implementation:**  \n",
        "- All lag features (lag_1, lag_7) were created using with only past rows allowed.as follows:\n",
        "    \n",
        "\n",
        "*   Written command line,windowSpec = Window.partitionBy(\"store_nbr\", \"item_nbr\").orderBy(\"date\"), used orderBy(\"date\"), prevents Spark from rows shuffling. which guarantees ordering according to time restriction\n",
        "*   partitionBy usage, means each store-item pair is its own separate time series.preventing stores data to leak through other stores and times,i.e\n",
        "data from other stores or other products never enters the calculation.\n",
        "*   At any row t, lag(\"unit_sales\", 1) uses only the value at t-1. Hence not using t or t+1 preventing future values to be used\n",
        "\n",
        "\n",
        "\n",
        "- Rolling features (7-day mean, 14-day std) were restricted to strictly **previous days only**, never including the current or future row, with the same way as previously explained, when using windowSpec.\n",
        "- Calendar features (day_of_week, is_weekend) come only from the `date` column, which is always known at prediction time, so no leakage risk.\n"
      ],
      "metadata": {
        "id": "4599-TzXu5Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assertions for Correctness of Generated Features**"
      ],
      "metadata": {
        "id": "oHoTc85OoE3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lag features assertion**\n",
        "This assertion verifies that lag features were generated correctly and without leakage by recalculating them independently and checking consistency. For each row, lag_1 must exactly match the previous day`s unit_sales, and lag_7 must exactly match the value from seven days earlier. If any mismatch is found, the assertion fails, ensuring that the lag features truly depend only on past data and not on the current or future values."
      ],
      "metadata": {
        "id": "_wN6EgdDmZAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For lag_1\n",
        "check_lag1 = (\n",
        "    df.withColumn(\"prev_sales\", F.lag(\"unit_sales\", 1).over(w))\n",
        "      .filter(F.col(\"lag_1\") != F.col(\"prev_sales\"))\n",
        ")\n",
        "assert check_lag1.count() == 0, \"lag_1 mismatch with actual pr evious row\"\n",
        "\n",
        "# For lag_7\n",
        "check_lag7 = (\n",
        "    df.withColumn(\"sales_7ago\", F.lag(\"unit_sales\", 7).over(w))\n",
        "      .filter(F.col(\"lag_7\") != F.col(\"sales_7ago\"))\n",
        ")\n",
        "assert check_lag7.count() == 0, \"lag_7 mismatch with actual 7th previous row\"\n"
      ],
      "metadata": {
        "id": "gf-frbBEwtjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rolling features assertion**\n",
        "This assertion manually validates the by comparing Spark`s computed values against Pandas ground truth on a small sample, for calculations reduction. For each row, it checks that rolling_mean_7 equals the average of the previous 7 rows and rolling_std_14 equals the standard deviation of the previous 14 rows, both excluding the current row. Pandas is used here because it makes it easy to compute and inspect these statistics row by row, serving as a reliable reference to confirm Spark’s results. If any mismatch is detected, the assertion fails, ensuring the rolling features are leakage-free."
      ],
      "metadata": {
        "id": "OiurS2UGm8mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Take a small sample for manual validation\n",
        "sample = (\n",
        "    df.filter((F.col(\"store_nbr\") == 1) & (F.col(\"item_nbr\") == 96995))\n",
        "      .orderBy(\"date\")\n",
        "      .limit(30)   # small sample for checking\n",
        "      .toPandas()\n",
        ")\n",
        "\n",
        "# Validate rolling features against Pandas ground truth\n",
        "for i in range(len(sample)):\n",
        "    # --- rolling mean (7 rows before current, exclude current row) ---\n",
        "    if i >= 7:\n",
        "        # Spark rowsBetween(-7, -1) → exactly the 7 rows before i\n",
        "        mean7 = sample.loc[i-7:i-1, \"unit_sales\"].mean()\n",
        "        assert math.isclose(\n",
        "            sample.loc[i, \"rolling_mean_7\"], mean7, rel_tol=1e-6\n",
        "        ), f\"Mismatch in rolling_mean_7 at row {i}\"\n",
        "\n",
        "    # --- rolling std (14 rows before current, exclude current row) ---\n",
        "    if i >= 14:\n",
        "        std14 = sample.loc[i-14:i-1, \"unit_sales\"].std(ddof=1)  # sample std (same as Spark)\n",
        "        assert math.isclose(\n",
        "            sample.loc[i, \"rolling_std_14\"], std14, rel_tol=1e-6\n",
        "        ), f\"Mismatch in rolling_std_14 at row {i}\"\n",
        "\n",
        "print(\"✅ Verified: rolling_mean_7 and rolling_std_14 match Spark's definition (no leakage).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt8T1NgFzhxO",
        "outputId": "d9b16d79-3b94-4c73-ae78-8bc27d7c8367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Verified: rolling_mean_7 and rolling_std_14 match Spark's definition (no leakage).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calendar-based features assertion\n",
        "This assertion validates calendar-based features to ensure they were generated correctly. The first check confirms that day_of_week values are always between 0 and 6 (Monday=0 to Sunday=6). The second check ensures that is_weekend is only set to 1 for Saturday and Sunday, and never for weekdays. These assertions guarantee that the date-derived features are consistent, logically correct, and free from errors."
      ],
      "metadata": {
        "id": "yBWMEPWPnPov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert df.filter(~F.col(\"day_of_week\").between(0,6)).count() == 0, \"Invalid day_of_week\"\n",
        "assert df.filter((F.col(\"day_of_week\") < 5) & (F.col(\"is_weekend\") == 1)).count() == 0, \"is_weekend wrong\"\n"
      ],
      "metadata": {
        "id": "NjUH4M_LkuDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Drop rows with nulls in any lag/rolling feature\n",
        "final_df = df.na.drop(\n",
        "    subset=[\"lag_1\", \"lag_7\", \"rolling_mean_7\", \"rolling_std_14\"]\n",
        ")\n",
        "\n",
        "# ✅ Save as partitioned Parquet by store\n",
        "(\n",
        "    final_df\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"store_nbr\")\n",
        "    .parquet(\"final_dataset_partitioned\")\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "SnIGSjL_o9z7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}